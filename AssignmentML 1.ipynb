{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "This is the first assgiment for the Coursera course \"Advanced Machine Learning and Signal Processing\"\n\nThe first step is to insert the credentials to the Apache CouchDB / Cloudant database where your sensor data ist stored to. \n\n1. In the project's overview tab of this project just select \"Add to project\"->Connection\n2. From the section \"Your service instances in IBM Cloud\" select your cloudant database and click on \"create\"\n3. Now click in the empty cell below labeled with \"your cloudant credentials go here\"\n4. Click on the \"10-01\" symbol top right and selecrt the \"Connections\" tab\n5. Find your data base connection and click on \"Insert to code\"\n\nThe following video illustrates this process: https://www.youtube.com/watch?v=dCawUGv7qgs\n\nDone, just execute all cells one after the other and you are done - just note that in the last one you have to update your email address (the one you've used for coursera) and obtain a submittion token, you get this from the programming assingment directly on coursera.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#your cloudant credentials go here\n# @hidden_cell\ncredentials_1 = {\n  'password':\"\"\"7e1eca3cee66d007b6b160458fc6e2cc2d7623a38dfae59f154a61f782702c84\"\"\",\n  'custom_url':'https://47640b73-1026-42e2-aa36-744120f1f020-bluemix:7e1eca3cee66d007b6b160458fc6e2cc2d7623a38dfae59f154a61f782702c84@47640b73-1026-42e2-aa36-744120f1f020-bluemix.cloudantnosqldb.appdomain.cloud',\n  'username':'47640b73-1026-42e2-aa36-744120f1f020-bluemix',\n  'url':'https://undefined'\n}\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 1
        }, 
        {
            "source": "spark = SparkSession\\\n    .builder\\\n    .appName(\"Cloudant Spark SQL Example in Python using temp tables\")\\\n    .config(\"cloudant.host\",credentials_1['custom_url'].split('@')[1])\\\n    .config(\"cloudant.username\", credentials_1['username'])\\\n    .config(\"cloudant.password\",credentials_1['password'])\\\n    .config(\"jsonstore.rdd.partitions\", 1)\\\n    .getOrCreate()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "df=spark.read.load('shake', \"com.cloudant.spark\")\n\ndf.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT * from df\").show()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+--------+----+----+----+--------------------+--------------------+\n|CLASS|SENSORID|   X|   Y|   Z|                 _id|                _rev|\n+-----+--------+----+----+----+--------------------+--------------------+\n|    0|tolstoi0|0.61|0.61|0.61|00b360c0ccf0777d2...|1-a9845528524ed0a...|\n|    0|tolstoi0|0.59|0.59|0.59|00b360c0ccf0777d2...|1-572b5c3dd1a43e2...|\n|    0|tolstoi0|0.58|0.58|0.58|00b360c0ccf0777d2...|1-f9221333016a218...|\n|    0|tolstoi0|0.56|0.56|0.56|00b360c0ccf0777d2...|1-b46a96e7f21311a...|\n|    0|tolstoi0|0.54|0.54|0.54|00b360c0ccf0777d2...|1-0068d54cd1af91a...|\n|    0|tolstoi0|0.54|0.54|0.54|00b360c0ccf0777d2...|1-0068d54cd1af91a...|\n|    0|tolstoi0|0.52|0.52|0.52|00b360c0ccf0777d2...|1-f4e81630df83bb5...|\n|    0|tolstoi0|0.52|0.52|0.52|00b360c0ccf0777d2...|1-f4e81630df83bb5...|\n|    0|tolstoi0|0.51|0.51|0.51|00b360c0ccf0777d2...|1-af2528179ec98c1...|\n|    0|tolstoi0|0.51|0.51|0.51|00b360c0ccf0777d2...|1-af2528179ec98c1...|\n|    0|tolstoi0|0.51|0.51|0.51|00b360c0ccf0777d2...|1-af2528179ec98c1...|\n|    0|tolstoi0| 0.5| 0.5| 0.5|00b360c0ccf0777d2...|1-01ca926d252f415...|\n|    0|tolstoi0|0.49|0.49|0.49|00b360c0ccf0777d2...|1-676a81c706877ea...|\n|    0|tolstoi0|0.51|0.51|0.51|00b360c0ccf0777d2...|1-af2528179ec98c1...|\n|    0|tolstoi0| 0.5| 0.5| 0.5|00b360c0ccf0777d2...|1-01ca926d252f415...|\n|    0|tolstoi0|0.58|0.58|0.58|00b360c0ccf0777d2...|1-f9221333016a218...|\n|    0|tolstoi0| 0.5| 0.5| 0.5|00b360c0ccf0777d2...|1-01ca926d252f415...|\n|    0|tolstoi0|0.49|0.49|0.49|00b360c0ccf0777d2...|1-676a81c706877ea...|\n|    0|tolstoi0| 0.5| 0.5| 0.5|00b360c0ccf0777d2...|1-01ca926d252f415...|\n|    0|tolstoi0| 0.5| 0.5| 0.5|00b360c0ccf0777d2...|1-01ca926d252f415...|\n+-----+--------+----+----+----+--------------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "!rm -Rf a2_m1.parquet", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "df = df.repartition(1)\ndf.write.json('a2_m1.json')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "'path file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s2b1-056f0dd30d96fa-f2ba7337ad24/notebook/work/a2_m1.json already exists.;'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o111.json.\n: org.apache.spark.sql.AnalysisException: path file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s2b1-056f0dd30d96fa-f2ba7337ad24/notebook/work/a2_m1.json already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:484)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:520)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:473)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:812)\n", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-5-a92573790cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a2_m1.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, mode, compression, dateFormat, timestampFormat)\u001b[0m\n\u001b[1;32m    616\u001b[0m         self._set_opts(\n\u001b[1;32m    617\u001b[0m             compression=compression, dateFormat=dateFormat, timestampFormat=timestampFormat)\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s2b1-056f0dd30d96fa-f2ba7337ad24/notebook/work/a2_m1.json already exists.;'"
                    ], 
                    "ename": "AnalysisException"
                }
            ], 
            "execution_count": 5
        }, 
        {
            "source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2019-01-23 20:26:34--  https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2289 (2.2K) [text/plain]\nSaving to: \u2018rklib.py\u2019\n\n100%[======================================>] 2,289       --.-K/s   in 0s      \n\n2019-01-23 20:26:34 (20.5 MB/s) - \u2018rklib.py\u2019 saved [2289/2289]\n\n"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "!zip -r a2_m1.json.zip a2_m1.json", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "updating: a2_m1.json/ (stored 0%)\nupdating: a2_m1.json/_SUCCESS (stored 0%)\nupdating: a2_m1.json/._SUCCESS.crc (stored 0%)\nupdating: a2_m1.json/part-00000-d908568c-81dc-47d4-ae4f-12ff6a5341f8.json (deflated 91%)\nupdating: a2_m1.json/.part-00000-d908568c-81dc-47d4-ae4f-12ff6a5341f8.json.crc (stored 0%)\n"
                }
            ], 
            "execution_count": 7
        }, 
        {
            "source": "!base64 a2_m1.json.zip > a2_m1.json.zip.base64", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 8
        }, 
        {
            "source": "from rklib import submit\nkey = \"1injH2F0EeiLlRJ3eJKoXA\"\npart = \"wNLDt\"\nemail = \"villegasjuan@yahoo.com\"\nsecret = \"3NQ3lM5cmT3BEF5m\"\n\nwith open('a2_m1.json.zip.base64', 'r') as myfile:\n    data=myfile.read()\nsubmit(email, secret, key, part, [part], data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Something went wrong, please have a look at the reponse of the grader\n-------------------------\n{\"errorCode\":\"invalidEmailOrToken\",\"message\":\"Invalid email or token.\",\"details\":{\"learnerMessage\":\"You used an invalid email or your token may have expired. Please make sure you have entered all fields correctly. Try generating a new token if the issue still persists.\"}}\n-------------------------\n"
                }
            ], 
            "execution_count": 9
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}